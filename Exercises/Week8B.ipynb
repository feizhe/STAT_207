{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Unsupervised Tumor Clustering via Expectation–Maximization (EM)\n",
    "\n",
    "Background & Motivation: Distinguishing cancerous (malignant) tumors from benign tumors is a critical healthcare task. Typically this is a supervised classification problem, but here we consider an unsupervised scenario: suppose we have biological measurements of tumors without known labels. Can we discover the underlying grouping of tumors into “malignant” vs “benign” purely from the data? This is a realistic situation when labeled data is scarce or expensive to obtain. We will use a Gaussian Mixture Model (GMM) and the Expectation–Maximization (EM) algorithm to cluster tumors, illustrating how EM can estimate parameters of latent-class models. This mirrors real-world use of EM in clustering, anomaly detection, and incomplete-data problems (e.g. grouping patients by disease subtype from symptoms). It also demonstrates EM’s power to handle latent variables (in this case, the unknown tumor class).\n",
    "\n",
    "## Wisconsin Diagnostic Breast Cancer (WDBC) dataset\n",
    "This is a well-known public dataset containing 569 tumor samples (212 malignant, 357 benign) described by 30 continuous features derived from cell nuclei images. Each sample is labeled M (malignant) or B (benign), though our clustering algorithm will not use these labels for training – we’ll only use them afterward to evaluate clustering performance. The features include radius, texture, area, smoothness of the cell nuclei, etc. ￼. This rich feature set makes the clusters (malignant vs benign) somewhat separable in the high-dimensional space, which EM should capitalize on. The real-world relevance is high: these measurements come from actual biopsies, and an unsupervised model that automatically separates likely malignant from benign tumors could assist in preliminary screening or insight into the data structure.\n",
    "\n",
    "## Problem Setup (GMM Formulation): \n",
    "\n",
    "We assume the data can be modeled as a mixture of two multivariate Gaussian distributions: one corresponding to benign tumors and one to malignant. Formally, let $x_i \\in \\mathbb{R}^{30}$ be the feature vector for the $i$-th tumor. We model the density as\n",
    "$$\n",
    "p(x_i \\mid \\Theta) = \\pi_1 \\mathcal{N}(x_i \\mid \\mu_1, \\Sigma_1) + \\pi_2 \\mathcal{N}(x_i \\mid \\mu_2, \\Sigma_2),\n",
    "$$\n",
    "where $\\Theta={\\pi_1,\\pi_2,\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2}$ are the model parameters. Here $\\pi_k$ is the mixing proportion (prior probability of cluster $k$, with $\\pi_1+\\pi_2=1$), $\\mu_k$ is the mean feature vector for cluster $k$, and $\\Sigma_k$ the $30\\times30$ covariance matrix. Initially, these parameters are unknown. Our goal is to learn $\\Theta$ that maximizes the likelihood of the observed data (i.e. fit the mixture to the data distribution). \n",
    "\n",
    "The log-likelihood is\n",
    "$$\n",
    "\\ell(\\Theta) = \\sum_{i=1}^{n} \\log\\Big[\\pi_1 \\mathcal{N}(x_i;\\mu_1,\\Sigma_1) + \\pi_2 \\mathcal{N}(x_i;\\mu_2,\\Sigma_2)\\Big].\n",
    "$$\n",
    "This is difficult to maximize directly because of the log-sum (the assignment of each $x_i$ to a component is unknown). However, if we introduce latent variables $z_i \\in {1,2}$ indicating the cluster identity of each point, the problem would simplify (complete-data log-likelihood is easier). EM tackles this by iteratively estimating the expected latent assignments (E-step) and then updating parameters to maximize that expected complete-data log-likelihood (M-step).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"target\")  # 0 = malignant, 1 = benign\n",
    "\n",
    "print(X.shape, y.value_counts())  # Check dimensions and class balance\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "\n",
    "- Lay out the details of using the EM algorithm to solve the GMM model.\n",
    "\n",
    "- Implement it in python and perform clustering on the cancer data. Do not use existing functions, but write your own code for the algorithm.\n",
    "\n",
    "- Evaluate the clustering result with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat_207",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
